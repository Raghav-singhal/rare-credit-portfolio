Particle methods depend upon the existence of a background Markov chain which is denoted by $X = {X_n}_{n\geq 0}$. This chain is not assumed to be time homogeneous. The random element $X_n$ takes values in some measurable state space $(E_n, \varepsilon_n)$ that can change with n. $K_n(x_{n-1}, dx_n)$ is the Markov transition kernels. $B_b(E)$ is the space of bounded measurable functions of the measurable space $(E,\varepsilon)$

\subsubsection{Feynman-Kac Path Expectations}
Let $Y_n$ be the history of $X_n$.
$$Y_n := (X_0,...,X_n) \in F_n := (E_0 \cross ... \cross E_n), n \geq 0$$

${Y_n}_{n\geq 0}$ is itself a Markov chain and we denote by $M_n(y_{n-1},dy_n)$ its transition kernel. For each $n \geq 0$, we choose a multiplicative potential function $G_n$ defined on $F_n$ and we define the Feynman-Kac expectations by

\begin{equation}
\gamma_n(f_n) = \mathbb{E} \left[ f(Y_n) \prod_{1\leq p<n} G_k(Y_p)\right]
\end{equation}

We define $\eta_n(.)$ the corresponding normalized measure defined as
\begin{equation}
\eta_n(f_n) = \frac{\mathbb{E}\left[ f_n(Y_n)\prod_{1\leq k<n}G_k(Y_k) \right]}{\mathbb{E}\left[\prod_{1\leq k<n}G_k(Y_k) \right]} = \gamma_n(f_n)/\gamma_n(1).
\end{equation}

We also observe that
$$\gamma_{n+1}(1) = \gamma_n(G_n) = \eta_n(G_n)\gamma_n(1) = \prod_{p=1}^{n}\eta_p(G_p)$$ 

Hence given any bounded measurable function $f_n$ on $F_n$ we have
$$\gamma_n(f_n) = \eta_n(f_n) \prod_{1\leq p<n}\eta_p(G_p)$$

Using the notation $G_p^- = 1/G_p$ and the above definitons of $\gamma_n$ and $\eta_n$ we see that

\begin{eqnarray}
\mathbb{E}[f_n(Y_n)] & = & \mathbb{E}\left[\ f_n(Y_n) \prod_{1\leq p < n} G_p^-(Y_p) \cross \prod_{1\leq p < n} G_p(Y_p) \right]\\
& = & \gamma_n\left(  f_n \prod_{1\leq p < n} G_p^- \right)\\
& = & \eta_n\left(  f_n \prod_{1\leq p < n} G_p^- \right) \prod_{1\leq p < n} \eta_p(G_p)
\end{eqnarray}

We can also check by inspection that the measures $(\eta_n)_{n\geq 1}$ satisfy the nonlinear recursive equation.
$$\eta_n = \varPhi_n(\eta_{n-1}) := \int_{F_{n-1}} \eta_{n-1}(dy_{n-1}) \frac{G_{n-1}(y_{n-1})}{\eta_{n-1}(G_{n-1})} M_n(y_{n-1},.)$$
starting from $\eta_1 = M_1(x_0,.)$. This dynamical equation on the space of measures is known as Stettners equation in filtering theory. We state it to justify the selection/mutation decomposition of each step of the particle algorithm introduced below.


\subsubsection{IPS Interpretation and Monte Carlo Algorithm}
We introduce a natural interacting path-particle system. For a given integer M, using the transformatios $\varPhi_n$, we construct a Markov chain $\{\xi_n\}_{n\geq 0}$ whose state $\xi_n = (\xi_n^j)_{1\leq j \leq M}$ at time n can be interpreted as a set of M Monte Carlo samples of path-particles 
$$\xi_n^j = (\xi_{0,n}^j, \xi_{1,n}^j,..., \xi_{n,n}^j) \in F_n = (E_0 \cross ... \cross E_n)$$
The transition mechanism of this Markov chain can be described as follows. We start with an initial configuration $\xi_1 = (\xi_1^j)_{1\leq j \leq M}$ that consists of M independent and identically distributed random variables with distribution,

$$\eta_1(d(y_0,y_1)) = M_1(x_0,d(y_0,y_1)) = \delta_{x_0}(dy_0)K_1(y_0, dy_1) $$
i.e $\xi_1^j := (\xi_{0,1}^j,\xi_{1,1}^j) = (x_0, \xi_{1,1}^j) \in F_1 = (E_0 \cross E_1)$ where the $\xi_{1,1}^j$ are drawn independently of each other from the distribution $K_1(x_0,.)$. Then the one-step transition taking $\xi_{n-1} \in F_{n-1}^M$ into $\xi \in F_n^M$ is given by a random draw from the distribution

\begin{equation}
\mathbb{P}\big( \xi_n \in d(y_n^1,...,y_n^M)|\xi_{n-1}  \big) = \prod_{j=1}^{M}\varPhi(m(\xi_{n-1}))(dy_n^j)
\end{equation}

where the notation $m(\xi_{n-1})$ is used for the empirical distribution of the $\xi_{n-1}^j$, i.e
$$m(\xi_{n-1}) := \frac{1}{M}\sum_{j=1}^{M}\delta_{\xi_{n-1}^{j}}$$

From the definition of $\varPhi_n$, we see that the above is the superposition of a section procedure followed by a mutation given by the transition of the original Markov chain. More precisely,
$$\xi_{n-1} \in F_{n-1}^{M}  \xrightarrow{\text{selection}} \hat{\xi}_{n-1} \in F_{n-1}^M \xrightarrow{\text{mutation}} \xi_n \in F_n^M$$

where the selection stage is performed by choosing randomly and independently M path-particles
$$\hat{\xi}_{n-1}^j = (\hat{\xi}_{0,n-1}^j, \hat{\xi}_{1,n-1}^j,...,\hat{\xi}_{n-1,n-1}^j) \in F_{n-1}$$
according to the Boltzmann-Gibbs measure
\begin{equation}
\sum_{j=1}^{M} \frac{G_{n-1}(\xi_{0,n-1}^j,...,\xi_{n-1,n-1}^j)}{\sum_{k=1}^{M}G_{n-1}(\xi_{0,n-1}^k,...,\xi_{n-1,n-1}^k)} \delta_{(\xi_{0,n-1}^j,...,\xi_{n-1,n-1}^j)}
\end{equation}
and for the mutation stage, each selected part-particle $\hat{xi}_{n-1}^j$ is extended as follows.
\begin{eqnarray*}
\xi_n^j & = & (\hat{\xi}_{n-1}^j, \xi_{n,n}^j)\\
& = & ((\hat{\xi}_{0,n-1}^j,..., \hat{\xi}_{n-1,n-1}^j), \xi_{n,n}^j) \in F_n = F_{n-1} \cross E_n
\end{eqnarray*}
where $\xi_{n,n}^j$ is a random variable with distribution $K_n(\hat{\xi}_{n-1,n-1},.)$. In other words, the transition step is a mere extension of the path particle with an element drawn at random using the transition kernel $K_n$ of the original Markov chain. All of the mutations are performed independently. But most importantly all of these mutations are happening with the original transition distribution of the chain. This is a contrast with importance sampling where the Monte Carlo transitions are from twisted transition distributions obtained from a Girsanov-like change of measure. So from a practical approach, a black-box providing random samples from the original chain transition distribution is enough for the implementation of the IPS algorithm.\\

Another result states that for each fixed time n, the empirical historical path measure
$$\eta_n^M := m(\xi_n) = \frac{1}{M} \sum_{j=1}^{M} \delta_{\xi_{0,n}^j,\xi_{1,n}^j,...,\xi_{n,n}^j}$$ converges in distribution as $M \rightarrow \infty$, toward the normalized Feynman-Kac measure $\eta_n$. Moreover there are several propagation of chaos estimates that ensure that $(\xi_{0,n}^j, \xi_{1,n}^j,...,\xi_{n,n}^j)$ are asymptotically independent and identically distributed with distribution $\eta_n$. This justifies for each measurable function $\tilde{f}_n$ on $F_n$ the choice of
\begin{equation}
\gamma_n^M(\tilde{f}_n) = \eta_n^M(\tilde{f}_n) \prod_{1\leq p <n}\eta_p^M(G_p)
\end{equation}
for a particle approximation of the expectation $\gamma_n(\tilde{f}_n)$

